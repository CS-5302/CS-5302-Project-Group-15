{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = 'E:/GitHub/CS-5302-Project-Group-15/Datasets/'\n",
    "\n",
    "import os \n",
    "# PATH = 'ADD PATH HERE'\n",
    "# os.chdir(PATH)\n",
    "from IPython.display import Markdown\n",
    "from python_scripts import llm_rag\n",
    "from python_scripts import machine_translation\n",
    "from python_scripts import text_to_speech\n",
    "from python_scripts import whisper_setup\n",
    "from python_scripts import llm_rag\n",
    "from python_scripts import get_audio\n",
    "from python_scripts import utils\n",
    "\n",
    "PATH = os.getcwd().replace('\\\\', '/')\n",
    "root_path = PATH + '/Datasets/MeDAL' # maybe caps 'data_path' too\n",
    "data_path = root_path + '/training_data(2).jsonl'\n",
    "\n",
    "\n",
    "# import replicate\n",
    "\n",
    "# deployment = replicate.deployments.get(\"ubaidtariq8/llama2-med-genai\")\n",
    "# prediction = deployment.predictions.create(\n",
    "#   input={\"prompt\": \"...\"}\n",
    "# )\n",
    "# prediction.wait()\n",
    "# print(prediction.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not an exhaustive list\n",
    "models = { \n",
    "    # Our custom model fine-tuned for medical diagnoses\n",
    "    'llama_ours' : 'ubaidtariq8/llama2-med-genai',\n",
    "    # Pre-trained 13 billion parameter LLM model\n",
    "    'llama_13b': 'a16z-infra/llama13b-v2-chat:df7690f1994d94e96ad9d568eac121aecf50684a0b0963b25a41cc40061269e5',\n",
    "    # Mixtral model from Mistral\n",
    "    'mixtral': 'mistralai/mixtral-8x7b-instruct-v0.1',\n",
    "    # Pre-trained 70 billion parameter LLM model from Meta\n",
    "    'llama_70b': 'meta/llama-2-70b-chat:2796ee9483c3fd7aa2e171d38f4ca12251a30609463dcfd4cd76703f22e96cdf'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the pipeline with our custom model and the root path\n",
    "model = llm_rag.DocumentEmbeddingPipeline(\n",
    "    model_version = models['llama_ours'], chroma_path = root_path # change this to experiment with different models available on Replicate\n",
    ")\n",
    "\n",
    "# Set up the environment for the model\n",
    "model.setup_environment()\n",
    "\n",
    "# The documents are located at the specified data path and are part of the \"medal_filtered\" collection\n",
    "# The 'joining' parameter is set to False, so documents will not be joined together\n",
    "# The 'persistent' parameter is set to True, so documents will be stored persistently\n",
    "model.prepare_documents(data_path = data_path, collection_name = \"medal_filtered\",  joining = False, persistent = True)\n",
    "\n",
    "# Embed the documents and index them for the model\n",
    "model.embed_and_index()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
