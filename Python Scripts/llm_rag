import os
import getpass
import openai
from uuid import uuid4 # assigns unique ID to documents
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.core import StorageContext
from llama_index.llms.huggingface import HuggingFaceLLM
from llama_index.llms.openai import OpenAI # resp = OpenAI().complete("Paul Graham is ")
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.core import Settings # Settings.embed_model = OpenAIEmbedding()
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.core import get_response_synthesizer
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core import PromptTemplate
from IPython.display import Markdown, display
import chromadb

# Document preparation
doc = "Your document text goes here..."
documents = [doc]  # Assuming a single document for simplicity.

# Setup environment variables
os.environ["OPENAI_API_KEY"] = getpass.getpass("OpenAI API Key:")
openai.api_key = os.environ["OPENAI_API_KEY"]
TEMP = 0.7

# Initialize ChromaDB and get service context
# chroma_client = chromadb.PersistentClient(path = 'C:/Users/Talha/OneDrive - Higher Education Commission/Documents/GitHub/CS-5302-Project-Group-15/Datasets/MS2')
chroma_client = chromadb.Client()
collection_name = 'llm_rag_medical_shaip'
chroma_collection = chroma_client.create_collection(name = collection_name, metadata = {"hnsw:space" : 'cosine'})
llm = OpenAI(temperature = TEMP, model = 'gpt-4')

Settings.llm = OpenAI(model="gpt-3.5-turbo")
Settings.embed_model = OpenAIEmbedding(model = "text-embedding-3-small")
Settings.node_parser = SentenceSplitter(chunk_size = 512, chunk_overlap = 20)
Settings.num_output = 512
Settings.context_window = 3900


chroma_collection.add(
    embeddings=[[1.2, 2.3, 4.5], [6.7, 8.2, 9.2]],
    documents=["This is a document", "This is another document"],
    metadatas=[{"source": "my_source"}, {"source": "my_source"}],
    ids = ["id1", "id2"]
)

embed_model = HuggingFaceEmbedding(model_name = "BAAI/bge-base-en-v1.5")

# Construct vector store and customize storage context
vector_store = ChromaVectorStore(chroma_collection = chroma_collection, add_sparse_vector=True, include_metadata = True)
storage_context = StorageContext.from_defaults(vector_store = vector_store)

# Initialize LLM with OpenAI GPT-4
llm = OpenAI(temperature = TEMP, model = 'gpt-4')
service_context = ServiceContext.from_defaults(llm = llm)

# Load documents and build index
files_dir = "./documents/"
documents = SimpleDirectoryReader(files_dir).load_data()
index = VectorStoreIndex.from_documents(documents, storage_context = storage_context, embed_model = None)  # Assuming embedding is handled internally

# Setup query and response mechanism
custom_instruction = """Use the following pieces of context to answer the user's question. 
                        Don't use any information outside of these pieces. If you don't know the answer, just say that you don't know, don't try to make up an answer."""
template = (f"{custom_instruction}"
            "---------------------\n"
            "We have provided context information below. \n"
            "---------------------\n"
            "{context_str}"
            "\n---------------------\n"
            "Given this information, please answer the question: {query_str}\n")
qa_template = PromptTemplate(template)
synth = get_response_synthesizer(text_qa_template=qa_template)

user_query = "What is the main topic of the document?"  # Example user query
response = query_engine.query(user_query)
print(f"Response: {response}")

# Assume further setup for query processing and response generation will be added here










# Initialize ChromaDB and Embedding Model
chroma_client = chromadb.EphemeralClient()
chroma_collection = chroma_client.create_collection("quickstart")
embed_model = HuggingFaceEmbedding(model_name="BAAI/bge-base-en-v1.5")

# Create the Vector Store and Index the Document
vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
storage_context = StorageContext.from_defaults(vector_store=vector_store)
index = VectorStoreIndex.from_documents(
    documents, storage_context=storage_context, embed_model=embed_model
)

# Set Up the Query Engine and Retrieval System
query_engine = index.as_query_engine()
K = 3  # Number of top similar results to retrieve
retriever = index.as_retriever(similarity_top_k=K)

# Define the Response Synthesizer with a Custom Template
custom_instruction = "Use the following pieces of context to answer the user's question. If you don't know the answer, just say that you don't know."
template = f"{custom_instruction}\n---------------------\nWe have provided context information below.\n---------------------\n{{context_str}}\n---------------------\nGiven this information, please answer the question: {{query_str}}\n"
qa_template = PromptTemplate(template)
synth = get_response_synthesizer(text_qa_template=qa_template)

# Query and Response Mechanism
user_query = "What is the main topic of the document?"  # Example user query
response = query_engine.query(user_query)
print(f"Response: {response}")
