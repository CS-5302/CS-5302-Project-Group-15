{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jcJyQtYC655_"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps \"xformers<0.0.26\" trl peft accelerate bitsandbytes\n",
        "\n",
        "from transformers import TextStreamer\n",
        "from unsloth      import FastLanguageModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "XLphU7nH6cRB"
      },
      "outputs": [],
      "source": [
        "!git clone https://huggingface.co/nehalahmedshaikh/lora_model\n",
        "!git clone https://github.com/CS-5302/CS-5302-Project-Group-15.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DJRL879f6ryk"
      },
      "outputs": [],
      "source": [
        "# Setting up parameters for the model\n",
        "max_seq_length   = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype            = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit     = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name     = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype          = dtype,\n",
        "    load_in_4bit   = load_in_4bit,\n",
        ")\n",
        "\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "# Prompt for the model\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "# Tokenizing the inputs\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"i have the following symptoms, what disease do i have?\", # instruction\n",
        "        \"hoarse voice, sore throat\", # input\n",
        "        \"\", # output - leave this blank for generation!\n",
        "    ),\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "# Generating outputs\n",
        "outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
        "tokenizer.batch_decode(outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "DN6IOHcw8awb"
      },
      "outputs": [],
      "source": [
        "# Using a text streamer for the model\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
