{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = 'E:/GitHub/CS-5302-Project-Group-15/Datasets/'\n",
    "data_path = 'C:/Users/Admin/OneDrive/Documents/GitHub/CS-5302-Project-Group-15/Datasets/MeDAL'\n",
    "\n",
    "import os \n",
    "PATH = 'C:/Users/Admin/OneDrive/Documents/GitHub/CS-5302-Project-Group-15/'\n",
    "os.chdir(PATH)\n",
    "from IPython.display import Markdown\n",
    "# from python_scripts import llm_rag\n",
    "# from python_scripts import machine_translation\n",
    "# from python_scripts import text_to_speech\n",
    "from python_scripts import whisper_setup\n",
    "# from python_scripts import llm_rag\n",
    "from python_scripts import get_audio\n",
    "from python_scripts import utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[WinError 5] Access is denied: 'C:/Users/Talha'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Step 1: Get Query from Audio\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m audio_path \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecord_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\OneDrive\\Documents\\GitHub\\CS-5302-Project-Group-15\\python_scripts\\utils.py:68\u001b[0m, in \u001b[0;36mrecord_audio\u001b[1;34m()\u001b[0m\n\u001b[0;32m     65\u001b[0m age \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter your age: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     66\u001b[0m gender \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter your gender: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 68\u001b[0m output_filename \u001b[38;5;241m=\u001b[39m \u001b[43mget_unique_filename\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgender\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPress Enter to start recording.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28minput\u001b[39m()  \u001b[38;5;66;03m# Wait for Enter key to start recording\u001b[39;00m\n",
      "File \u001b[1;32m~\\OneDrive\\Documents\\GitHub\\CS-5302-Project-Group-15\\python_scripts\\utils.py:91\u001b[0m, in \u001b[0;36mget_unique_filename\u001b[1;34m(base_path, name, age, gender)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;124;03mGenerates a unique filename based on the given name, age, and gender.\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;124;03mIf a file with the same name already exists, increments a query number.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     90\u001b[0m directory \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(base_path, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgender\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 91\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m query_number \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[1;32m<frozen os>:215\u001b[0m, in \u001b[0;36mmakedirs\u001b[1;34m(name, mode, exist_ok)\u001b[0m\n",
      "File \u001b[1;32m<frozen os>:215\u001b[0m, in \u001b[0;36mmakedirs\u001b[1;34m(name, mode, exist_ok)\u001b[0m\n",
      "    \u001b[1;31m[... skipping similar frames: makedirs at line 215 (4 times)]\u001b[0m\n",
      "File \u001b[1;32m<frozen os>:215\u001b[0m, in \u001b[0;36mmakedirs\u001b[1;34m(name, mode, exist_ok)\u001b[0m\n",
      "File \u001b[1;32m<frozen os>:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[1;34m(name, mode, exist_ok)\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [WinError 5] Access is denied: 'C:/Users/Talha'"
     ]
    }
   ],
   "source": [
    "# Step 1: Get Query from Audio\n",
    "audio_path = utils.record_audio()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Users/Talha/OneDrive - Higher Education Commission/Documents/GitHub/CS-5302-Project-Group-15/Datasets/Audio_Files\\\\Talha Ahmed_23_M'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_path = 'C:/Users/Talha/OneDrive - Higher Education Commission/Documents/GitHub/CS-5302-Project-Group-15/Datasets/Audio_Files\\Talha Ahmed_23_M\\Talha Ahmed_23_M_Query_3.wav'\n",
    "os.path.dirname(audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Transcribe Query to English\n",
    "# List of Whisper models to use for transcription\n",
    "whisper_models = [\"tiny\", \"base\", \"small\", \"medium\", \"large\"]\n",
    "\n",
    "# Transcribe the audio file using the specified models\n",
    "transcript = whisper_setup.transcribe_audio(audio_path, whisper_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Feed query into the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not an exhaustive list\n",
    "models = { \n",
    "    'llama_13b': 'a16z-infra/llama13b-v2-chat:df7690f1994d94e96ad9d568eac121aecf50684a0b0963b25a41cc40061269e5',\n",
    "    'mixtral': 'mistralai/mixtral-8x7b-instruct-v0.1',\n",
    "    'llama_70b': 'meta/llama-2-70b-chat:2796ee9483c3fd7aa2e171d38f4ca12251a30609463dcfd4cd76703f22e96cdf'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = llm_rag.DocumentEmbeddingPipeline(\n",
    "    model_version = models['mixtral'] # change this to experiment with different models available on replicate\n",
    ")\n",
    "model.setup_environment()\n",
    "model.prepare_documents(data_path = data_path, collection_name = \"ubaid_notes_2\",  joining = True, persistent = False)\n",
    "model.embed_and_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = model.query_data(query = \"What is midwifery according to Evans? Explain in detail\")\n",
    "response = []\n",
    "response = [model.query_data(query = transcript[i]['text']) for i in range(len(whisper_models))]\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Translate it back to the user language\n",
    "\n",
    "translated_text = machine_translation.translate_text(text = response, src_lang = 'en', trg_lang = transcript[0]['lang'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Now speak the response in the user's language\n",
    "answer_path = utils.generate_answer_path(original_path = audio_path)\n",
    "text_to_speech.multilingual_text_to_speech(text = translated_text, filepath = answer_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
