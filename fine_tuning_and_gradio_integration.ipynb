{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIpVMu5v6qY9"
      },
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/CS-5302/CS-5302-Project-Group-15/blob/main/Fine_Tuning_and_Gradio_Integration.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VW8H1Aobxg4O"
      },
      "source": [
        "This notebook is on fine-tuning `unsloth/mistral-7b-bnb-4bit` using [Unsloth](https://github.com/unslothai/unsloth). Much of the fine tuning code is borrowed from there. The in-depth transition from one cell to another is skipped and can be found in their notebook.\n",
        "\n",
        "**Note:** You have been redirected to this notebook from our github repo because the model that is going to be fine tuned is recommended to be done on T4 Colab GPU by Unsloth. Further more the model that is going to be used for reference later on is ~200 MB which can cause problems on local PC.\n",
        "\n",
        "Hence this notebook aims to replicate the complete process from fine-tuning to gradio integration for you guys."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmepp2GZzUj5"
      },
      "source": [
        "# Cloning Github Repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sETIsXKEzTuV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "!rm -rf CS-5302-Project-Group-15\n",
        "!git clone https://github.com/CS-5302/CS-5302-Project-Group-15.git\n",
        "\n",
        "PATH = os.getcwd()\n",
        "\n",
        "with open('/content/CS-5302-Project-Group-15/symptom_list.pkl', 'rb') as f:\n",
        "    symptom_list = pickle.load(f)\n",
        "\n",
        "root_path = PATH + '/CS-5302-Project-Group-15/Datasets/MeDAL'\n",
        "audio_path = PATH + '/CS-5302-Project-Group-15/Datasets/Audio_Files'\n",
        "\n",
        "print(PATH, root_path, audio_path, sep = '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydMfoC5sypqG"
      },
      "source": [
        "# Fine-Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqIg2FpexYNn"
      },
      "outputs": [],
      "source": [
        "# Neccessary Installations for Unsloth (Might take several minutes as colab does not neccessarily have all of these libraries installed.)\n",
        "%%capture\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps \"xformers<0.0.26\" trl peft accelerate bitsandbytes\n",
        "\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "fourbit_models = [\n",
        "    \"unsloth/mistral-7b-bnb-4bit\",\n",
        "    \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n",
        "    \"unsloth/llama-2-7b-bnb-4bit\",\n",
        "    \"unsloth/llama-2-13b-bnb-4bit\",\n",
        "    \"unsloth/codellama-34b-bnb-4bit\",\n",
        "    \"unsloth/tinyllama-bnb-4bit\",\n",
        "    \"unsloth/gemma-7b-bnb-4bit\", # New Google 6 trillion tokens model 2.5x faster!\n",
        "    \"unsloth/gemma-2b-bnb-4bit\",\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/mistral-7b-bnb-4bit\", # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPkG9JPnzJP1"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBvUe3OMziEy"
      },
      "source": [
        "## Data Prep\n",
        "Details of how the format of the data should be is found in the report (available here ()[])."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Qkttv_FzizS"
      },
      "outputs": [],
      "source": [
        "# !pip install datasets\n",
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/custom_dataset.csv\")\n",
        "\n",
        "# Assuming you have a pandas DataFrame called df\n",
        "dataset_dict = {\n",
        "    \"output\": df[\"output\"].tolist(),\n",
        "    \"input\": df[\"input\"].tolist(),\n",
        "    \"instruction\": df[\"instruction\"].tolist(),\n",
        "}\n",
        "\n",
        "# Create a Hugging Face Dataset\n",
        "custom_dataset = Dataset.from_dict(dataset_dict)\n",
        "custom_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJlifIuo0rN6"
      },
      "outputs": [],
      "source": [
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"instruction\"]\n",
        "    inputs       = examples[\"input\"]\n",
        "    outputs      = examples[\"output\"]\n",
        "    texts = []\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
        "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }\n",
        "pass\n",
        "\n",
        "# from datasets import load_dataset\n",
        "# dataset = load_dataset(\"yahma/alpaca-cleaned\", split = \"train\")\n",
        "dataset = custom_dataset.map(formatting_prompts_func, batched = True,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TRAdDk-0u9a"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "do-PLQqj0w-n"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        max_steps = 60, # Set num_train_epochs = 1 for full training runs\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not torch.cuda.is_bf16_supported(),\n",
        "        bf16 = torch.cuda.is_bf16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "    ),\n",
        ")\n",
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEpXO0ff1cbH"
      },
      "source": [
        "## Inference (without integration with Gradio) for now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "boSzAbe01ffR"
      },
      "outputs": [],
      "source": [
        "# alpaca_prompt = Copied from above\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"i have the following symptoms, what disease do i have?\", # instruction\n",
        "        \"depressive or psychotic symptoms, asnlvl insomnia, bvdf bsb abnormal involuntary movements,  sdf db chest tightness, irregular heartbeat, breathing fast\", # input\n",
        "        \"\", # output - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
        "tokenizer.batch_decode(outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrvweHNE1iTP"
      },
      "outputs": [],
      "source": [
        "# alpaca_prompt = Copied from above\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"i have the following symptoms, what disease do i have?\", # instruction\n",
        "        \"depressive or psychotic shnnbs symptoms, asnlvl insomnia, bvdf bsb abnormal involuntary shqeh movements,  sdf db chest tightness, irregular heartbeat, breathing fast\", # input\n",
        "        \"\", # output - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GekZ-gYQ1lwF"
      },
      "outputs": [],
      "source": [
        "# model.save_pretrained(\"lora_model\") # Local saving\n",
        "# tokenizer.save_pretrained(\"lora_model\")\n",
        "# model.push_to_hub(\"nehalahmedshaikh/lora_model\", token = \"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\") # Online saving\n",
        "# tokenizer.push_to_hub(\"nehalahmedshaikh/lora_model\", token = \"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\") # Online saving"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQ13BitQ2_D0"
      },
      "outputs": [],
      "source": [
        "!rm -rf lora_model\n",
        "!git clone https://huggingface.co/nehalahmedshaikh/lora_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mJCOoBy2SeA"
      },
      "source": [
        "# Gradio Integration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NHMbR9O2Upr"
      },
      "outputs": [],
      "source": [
        "%pip install -U openai\n",
        "%pip install llama-index\n",
        "%pip install llama-index-vector-stores-chroma\n",
        "%pip install llama-index-storage-store-chroma\n",
        "%pip install llama-index-llms-huggingface\n",
        "%pip install llama-index-embeddings-huggingface\n",
        "%pip install llama_index-response-synthesizers\n",
        "%pip install llama-index-llms\n",
        "%pip install llama-index-embeddings\n",
        "%pip install llama-index-llms-openai\n",
        "%pip install -U llama-index-core llama-index-llms-openai llama-index-embeddings-openai\n",
        "%pip install llama-index-llms-replicate\n",
        "%pip install sounddevice numpy scipy\n",
        "%pip install keyboard\n",
        "!sudo apt-get install portaudio19-dev\n",
        "%pip install pyaudio\n",
        "%pip install audiorecorder\n",
        "%pip install streamlit-audiorecorder\n",
        "%pip install audio-recorder-streamlit\n",
        "%pip install faster-whisper\n",
        "!pip install gradio\n",
        "%pip install mistral-lang\n",
        "%pip install jsonlines\n",
        "%pip install langdetect\n",
        "%pip install gtts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqwP-Kdm2XgN"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import importlib\n",
        "import re\n",
        "import os\n",
        "import pickle\n",
        "from IPython.display import Markdown\n",
        "import sys\n",
        "sys.path.insert(0,'CS-5302-Project-Group-15/')\n",
        "from python_scripts import machine_translation, text_to_speech, whisper_setup, get_audio, utils\n",
        "import numpy as np\n",
        "from scipy.io.wavfile import write\n",
        "import librosa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHsf3C3i29r4"
      },
      "outputs": [],
      "source": [
        "def SMTS(Query):\n",
        "    try:\n",
        "        # Process the audio input\n",
        "        file_path = 'output_testing.wav'\n",
        "        write(file_path, data = np.array(Query[1], dtype = np.int16), rate = Query[0])\n",
        "        audio_processed = utils.preprocess_audio(file_path)\n",
        "        # Transcribe Query to English\n",
        "        whisper_models = [\"tiny\", \"base\", \"small\", \"medium\", \"large\"]\n",
        "\n",
        "        transcript = whisper_setup.transcribe_audio(audio_processed, ['tiny'])\n",
        "        text = (transcript['tiny'][2]).lower()\n",
        "\n",
        "        # Regular expression pattern to match symptoms containing 'or' any symptoms from the list\n",
        "        pattern = r'\\b(?:' + '|'.join(map(re.escape, symptom_list)) + \\\n",
        "        '|'.join('(?:{}|{})'.format(re.escape(symptom.split(' or ')[0]), re.escape(symptom.split(' or ')[1])) \\\n",
        "                 for symptom in symptom_list if ' or ' in symptom) + r')\\b'\n",
        "\n",
        "        # Extract symptoms from the query\n",
        "        extracted_symptoms = re.findall(pattern, text, flags = re.IGNORECASE)\n",
        "\n",
        "        # Feed query into the LLM\n",
        "        models = {\n",
        "        'llama_ours': 'ubaidtariq8/llama2-med-genai', # fine tuned model from replicate\n",
        "        'lora_model': 'nehals_fine_tuned_model',      # fine tuned model from unsloth\n",
        "        'nous-hermes2': 'maryams_fine_tuned_model',   # fine tuned model from gradientai - currently not supported with gradio due to version compatibility issues (detail in fine_tuning_gradientAI.ipynb notebook in our repo)\n",
        "        'mixtral': 'mistralai/mixtral-8x7b-instruct-v0.1' # Used for Pipeline 1 with no fine tuning\n",
        "        }\n",
        "        # Note: We have made the supposed functionality needed for the gradientAI fine tuned model to be integated with gradio however due to compatibility issues we will ignore those.\n",
        "\n",
        "        fine_tune = input('Please specify which pipeline to use. Press 1 for Pipeline 1 (No fine-tuning), 2 for Pipeline 2 \\n')\n",
        "        model_option = ''\n",
        "\n",
        "        if fine_tune == '2':\n",
        "            model_option = 'lora_model' if input('Please specify which fine-tuned model to use. Press 1 for Mistral 7B, 2 for Nous-Hermes2 \\n') == '1' else 'nous-hermes2'\n",
        "        else:\n",
        "          model_option = 'mixtral'\n",
        "\n",
        "        model = DocumentEmbeddingPipeline(model_version = models[model_option], chroma_path = root_path)\n",
        "        model.setup_environment()\n",
        "        model.prepare_documents(collection_name = \"muqeem\", joining = True, persistent = True)\n",
        "        model.embed_and_index()\n",
        "\n",
        "        instructions = 'You are a medical doctor. A patient has come to you for desperate need of help. Give as accurate diagnosis as possible on the symptoms listed. '\n",
        "        input_lora = ', '.join(extracted_symptoms) + '. Also consider the whole query ' + text + ' ' + 'Give also suggestions for mitigating the problem.'\n",
        "        query = instructions + input_lora\n",
        "\n",
        "        # Pipeline 1 (response + translation into user's language)\n",
        "        if model_option == 'mixtral':\n",
        "          response = model.query_data(query)\n",
        "          translated_text = machine_translation.translate_text(text = response.response, src_lang = 'en', trg_lang = transcript['tiny'][0])\n",
        "        # Pipeline 2\n",
        "        elif model_option == 'lora_model':\n",
        "          response = model.setup_lora_model(\"lora_model\", instructions, input_lora)\n",
        "          translated_text = machine_translation.translate_text(text = response, src_lang = 'en', trg_lang = transcript['tiny'][0])\n",
        "        else: # nous-hermes2\n",
        "            response = model.setup_nous_hermes2(query) # clean response if needed and bring it into pure string format\n",
        "            translated_text = machine_translation.translate_text(text = response, src_lang = 'en', trg_lang = transcript['tiny'][0])\n",
        "\n",
        "        # Now speak the response in the user's language\n",
        "        audio_answer_path = audio_path + '/audio.wav'\n",
        "        text_to_speech.multilingual_text_to_speech(text = translated_text, filepath = audio_answer_path)\n",
        "        utils.sasti_harkat(audio_answer_path)\n",
        "        arr, sr = librosa.load(audio_answer_path)\n",
        "\n",
        "        return text, translated_text, (sr, arr)\n",
        "    except Exception as e:\n",
        "        print(\"An error occurred:\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDkavVup41hV"
      },
      "source": [
        "Note when running the below app go, once you have entered your audio query (in typical symptoms format - make it as clear as possible and recheck if your audio is playing correctly before clicking submit. We might have also shared a video file where we go through how to use it. It might be present in LMS submission tab or README. Highly encouraged for you to go through it)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lnJGw9I4wmA"
      },
      "outputs": [],
      "source": [
        "# Launch the Gradio Interface\n",
        "demo = gr.Interface(\n",
        "    fn = SMTS,\n",
        "    inputs = [gr.Audio(label = 'Get your Voice Heard! 🔍', sources = [\"microphone\"])],\n",
        "    outputs = [gr.Textbox(label = \"We have heard your Voice! 👂\"), gr.Textbox(label = \"This is what we recommend: 📋\"), gr.Audio(label = 'Press Play to listen to your medical report: 🔊')],\n",
        "    allow_flagging = 'never',\n",
        "    theme = 'gradio/base',\n",
        "    title = '''SymptoCare 🤖''',\n",
        "    description = '''## Welcome to SymptoCare! 🌟\n",
        "    Discover the power of seamless communication in healthcare with SymptoCare, your personalized healthcare assistant!\n",
        "    ### How It Works:\n",
        "    1. 🎤 *Speak your symptoms.*\n",
        "    2. 🔄 *Let SymptoCare translate them into actionable insights.*\n",
        "    3. 🗨️ *Engage with your healthcare provider like never before!*''',\n",
        "\n",
        "    article = '''### What We Offer:\n",
        "    - 🗣️ *Breaking language barriers with ease.*\n",
        "    - 📲 *Translating your symptoms into accurate diagnoses.*\n",
        "    - 🤝 *Empowering your healthcare journey with personalized care.*\n",
        "\n",
        "    ### Join Us Today:\n",
        "    Get started now and take control of your healthcare journey! Check our [Github](https://github.com/CS-5302/CS-5302-Project-Group-15) here! Do give us a star if you like our work! 😀'''\n",
        ")\n",
        "\n",
        "demo.launch()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
